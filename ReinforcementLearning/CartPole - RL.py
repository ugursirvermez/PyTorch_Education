# -*- coding: utf-8 -*-
"""13_pytorch_reinforcement_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tiFBtfwcfYZqkBxqgSLM3BL85_oHNI9S

# Reinforcement Learning

Ä°Ã§erik Sahibi: Adam Paszke <br>

[Linkten orijinal](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/9da0471a9eeb2351a488cd4b44fc6bbf/reinforcement_q_learning.ipynb) metine ulaÅŸabilirsiniz. <br>

Agent, iki eylem arasÄ±nda karar vermek zorundadÄ±r - arabayÄ± sola veya saÄŸa hareket ettirmek - bÃ¶ylece ona baÄŸlÄ± direk dik kalÄ±r. Gymnasium'un web sitesinde ortam ve diÄŸer daha zorlu ortamlar hakkÄ±nda daha fazla bilgi bulabilirsiniz. Ajan ortamÄ±n mevcut durumunu gÃ¶zlemledikÃ§e ve bir eylem seÃ§tikÃ§e, ortam yeni bir duruma geÃ§er ve ayrÄ±ca eylemin sonuÃ§larÄ±nÄ± gÃ¶steren bir Ã¶dÃ¼l dÃ¶ndÃ¼rÃ¼r. Bu gÃ¶revde, Ã¶dÃ¼ller her artÄ±mlÄ± zaman adÄ±mÄ± iÃ§in +1'dir ve direk Ã§ok fazla devrilirse veya araba merkezden 2,4 birimden fazla uzaklaÅŸÄ±rsa ortam sonlanÄ±r. Bu, daha iyi performans gÃ¶steren senaryolarÄ±n daha uzun sÃ¼re Ã§alÄ±ÅŸacaÄŸÄ± ve daha bÃ¼yÃ¼k getiri elde edeceÄŸi anlamÄ±na gelir.

CartPole gÃ¶revi, agent'e girdilerin ortam durumunu (konum, hÄ±z vb.) temsil eden 4 gerÃ§ek deÄŸer olacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r. Bu 4 girdiyi herhangi bir Ã¶lÃ§ekleme yapmadan alÄ±rÄ±z ve her eylem iÃ§in bir tane olmak Ã¼zere 2 Ã§Ä±ktÄ±sÄ± olan kÃ¼Ã§Ã¼k, tamamen baÄŸlÄ± bir aÄŸdan geÃ§iririz. AÄŸ, girdi durumu verildiÄŸinde her eylem iÃ§in beklenen deÄŸeri tahmin edecek ÅŸekilde eÄŸitilir. Daha sonra en yÃ¼ksek beklenen deÄŸere sahip eylem seÃ§ilir.

https://gist.github.com/hatashiro/bb7584e890300b633339a3c09281d044
"""

"""##OrtamÄ±n KurulmasÄ±"""

import gymnasium as gym #RL iÃ§in kullanÄ±lan, Ã§evre oluÅŸturmayÄ± saÄŸlayan bir kÃ¼tÃ¼phanedir.
import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt #Grafik
from collections import namedtuple, deque #Verileri listelemek iÃ§in
from itertools import count
from PIL import Image
import glob
import io
import os
import base64
#PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T
#GÃ¶rselleÅŸtirme iÃ§in
import pyvirtualdisplay
from IPython import display as ipythondisplay
# matplotlib Kurulumu
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display
    from IPython.display import HTML

###### Environment #######
#Environment seÃ§imi -> Oyunun adÄ±nÄ± yazÄ±yoruz.
env = gym.make("CartPole-v1",render_mode="rgb_array")
plt.ion()

#EkranÄ± BaÅŸlatma
if not os.environ.get('DISPLAY'):
    pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()

# GPU mu yoksa CPU mu?
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""## Video KayÄ±t"""

def render_as_image(env):
    img = env.render()
    img = Image.fromarray(img)
    plt.imshow(img)
    plt.axis('off')
    plt.show()

def embed_video(video_path, width=640):
    content = io.open(video_path, 'rb').read()
    b64 = base64.b64encode(content)
    video_tag = '''

    '''.format(b64.decode(), width)
    ipythondisplay.display(ipythondisplay.HTML(video_tag))

import imageio
from datetime import datetime

def random_filename():
    return datetime.now().strftime('%Y_%m_%d_%H_%M_%S.mp4')

class VideoRecorder:
    def __init__(self, filename=random_filename(), fps=30):
        self.filename = filename
        self.writer = imageio.get_writer(filename, fps=fps)

    def record_frame(self, env):
        frame = env.render()
        self.writer.append_data(frame)

    def close(self, *args, **kwargs):
        self.writer.close(*args, **kwargs)

    def play(self):
        self.close()
        embed_video(self.filename)

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        self.play()

"""## TekrarlÄ± HafÄ±za OluÅŸturma"""

#Bu deÄŸiÅŸken birden fazla etikete sahip.
#Etiketler -> Durum, eylem, sonraki durum ve Ã¶dÃ¼lden oluÅŸuyor.
Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))

#Agent'Ä±n yaptÄ±ÄŸÄ± eylemler sonucunda hafÄ±zasÄ±nÄ± oluÅŸturuyoruz.
class ReplayMemory(object):

    #HafÄ±zanÄ±n kapasitesini belirliyoruz.
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)
    #Her bir veriyi transition adlÄ± deÄŸiÅŸkene aktarÄ±yoruz.
    def push(self, *args):
        self.memory.append(Transition(*args))
    #Verileri karÅŸÄ±laÅŸtÄ±rmak iÃ§in hafÄ±zamÄ±zdan batch miktarÄ±
    #kadar veriyi rastgele seÃ§iyoruz.
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    #HafÄ±za miktarÄ± ne kadar oldu bunu dÃ¶ndÃ¼ren fonksiyon
    def __len__(self):
        return len(self.memory)

"""## DQN AlgoritmasÄ±
Ã‡evremiz deterministiktir, bu nedenle burada sunulan tÃ¼m denklemler basitlik adÄ±na deterministik olarak formÃ¼le edilmiÅŸtir. Takviyeli Ã¶ÄŸrenme literatÃ¼rÃ¼nde, Ã§evredeki stokastik geÃ§iÅŸler Ã¼zerindeki beklentileri de iÃ§erirler. AmacÄ±mÄ±z, iskontolu, kÃ¼mÃ¼latif Ã¶dÃ¼lÃ¼ maksimize etmeye Ã§alÄ±ÅŸan bir politika eÄŸitmek olacaktÄ±r, **ğ‘…ğ‘¡0=âˆ‘âˆğ‘¡=ğ‘¡0ğ›¾ğ‘¡âˆ’ğ‘¡0ğ‘Ÿğ‘¡ **, burada ğ‘…ğ‘¡0 aynÄ± zamanda getiri olarak da bilinir. Ä°skonto, ğ›¾ , toplamÄ±n yakÄ±nsamasÄ±nÄ± saÄŸlayan 0 ile 1 arasÄ±nda bir sabit olmalÄ±dÄ±r. Daha dÃ¼ÅŸÃ¼k bir ğ›¾ , belirsiz uzak gelecekten gelen Ã¶dÃ¼lleri, hakkÄ±nda oldukÃ§a emin olabileceÄŸi yakÄ±n gelecekteki Ã¶dÃ¼llerden daha az Ã¶nemli hale getirir. AyrÄ±ca, temsilcileri, gelecekte zamansal olarak uzak olan eÅŸdeÄŸer Ã¶dÃ¼llerden daha yakÄ±n zamanda Ã¶dÃ¼l toplamaya teÅŸvik eder. <br>

Q-Ã¶ÄŸrenmesinin arkasÄ±ndaki temel fikir, eÄŸer **ğ‘„:ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’Ã—ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†’â„** ÅŸeklinde bir fonksiyonumuz olsaydÄ± ve bu fonksiyon bize belirli bir durumda bir eylemde bulunursak getirimizin ne olacaÄŸÄ±nÄ± sÃ¶yleyebilseydi, Ã¶dÃ¼llerimizi en Ã¼st dÃ¼zeye Ã§Ä±karacak bir politikayÄ± kolayca oluÅŸturabilirdik:
### ğœ‹âˆ—(ğ‘ )=argmaxğ‘ ğ‘„âˆ—(ğ‘ ,ğ‘) <br>

### Q-network
Modelimiz, geÃ§erli ve Ã¶nceki ekran yamalarÄ± arasÄ±ndaki farkÄ± alan ileri beslemeli bir sinir aÄŸÄ± olacak. ğ‘„(ğ‘ ,sol) ve ğ‘„(ğ‘ ,saÄŸ)'Ä± temsil eden iki Ã§Ä±ktÄ±sÄ± var (burada ğ‘ , aÄŸÄ±n girdisidir). AslÄ±nda, aÄŸ, geÃ§erli girdi verildiÄŸinde her eylemin beklenen getirisini tahmin etmeye Ã§alÄ±ÅŸmaktadÄ±r.
"""

#DQN SÄ±nÄ±fÄ± OluÅŸturarak Veri Analizi YapÄ±lacak
class DQN(nn.Module):
    #GÃ¶zlemler ve eylemler sonucunda doÄŸrusal bir veri azalmasÄ± olmalÄ±.
    #Bu veri azalmasÄ± eylem ve durumun bir orantÄ±sÄ± ile saÄŸlanabilir.
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    # Bir sonraki eylemi belirlemek iÃ§in bir Ã¶ÄŸeyle veya
    # optimizasyon sÄ±rasÄ±nda bir toplu iÅŸlemle Ã§aÄŸrÄ±lÄ±r.
    # Tensor([[left0exp,right0exp]...]) dÃ¶ndÃ¼rÃ¼r.
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

"""## EÄŸitim"""

BATCH_SIZE = 128 # tekrar oynatma arabelleÄŸinden Ã¶rneklenen geÃ§iÅŸlerin sayÄ±sÄ±dÄ±r
GAMMA = 0.99 # Ã¶nceki bÃ¶lÃ¼mde belirtildiÄŸi gibi indirim faktÃ¶rÃ¼dÃ¼r
EPS_START = 0.9 # EPS_START epsilon'un baÅŸlangÄ±Ã§ â€‹â€‹deÄŸeridir
EPS_END = 0.05 # EPS_END epsilon'un son deÄŸeridir.
EPS_DECAY = 1000 # epsilonun Ã¼stel azalma oranÄ±nÄ± kontrol eder; daha yÃ¼ksek deÄŸer daha yavaÅŸ azalma anlamÄ±na gelir
TAU = 0.005 # hedef aÄŸÄ±n gÃ¼ncelleme oranÄ±dÄ±r
LR = 1e-4 # Ã–ÄŸrenme oranÄ± Adam optimizer'a gÃ¶re

#OrtamÄ±n GÃ¶rselini Ã§Ä±karma
env.reset()
render_as_image(env)

# Ortam eylem sayÄ±sÄ±nÄ± alma
n_actions = env.action_space.n
# Durum gÃ¶zlemlerini alma
state, info = env.reset()
n_observations = len(state)

#Hedef ve uygulanacak protokol deÄŸerlerini alma
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())
#Optimizer
optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(10000) #Ne kadarlÄ±k hafÄ±za oluÅŸacak?

#KaÃ§ adÄ±m yapÄ±ldÄ±
steps_done = 0

#Eylem SeÃ§imi
def select_action(state):
    global steps_done
    sample = random.random()
    #YukarÄ±daki formÃ¼l uygulanÄ±yor.
    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    if sample > eps_threshold:
        with torch.no_grad():
            #En yÃ¼ksek deÄŸere ulaÅŸmak iÃ§in eylemlerdeki Ã¶dÃ¼lleri takip ediyoruz.
            return policy_net(state).max(1).indices.view(1, 1)
    else:
        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)

#Devirlerin sÃ¼resi
episode_durations = []

#Devir sÃ¼relerini grafikleÅŸtirme
def plot_durations(show_result=False):
    plt.figure(1)
    durations_t = torch.tensor(episode_durations, dtype=torch.float)
    if show_result:
        plt.title('Result')
    else:
        plt.clf()
        plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('Duration')
    plt.plot(durations_t.numpy())
    # 100 tane olmalÄ± en az
    if len(durations_t) >= 100:
        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy())

    plt.pause(0.001)  # grafik gÃ¼ncelleme sÃ¼resi
    if is_ipython:
        if not show_result:
            display.display(plt.gcf())
            display.clear_output(wait=True)
        else:
            display.display(plt.gcf())

"""## EÄŸitim DÃ¶ngÃ¼sÃ¼nÃ¼ OluÅŸturma"""

#Modeli Ä°yileÅŸtirme
def optimize_model():
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions)) #Durumlar, eylemler, sonraki durumlar ve Ã¶dÃ¼ller

    # BÃ¼tÃ¼n durumlarÄ± bir araya getirme. Tuple ile.
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                          batch.next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                if s is not None])
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    #DurumlarÄ± gÃ¶zlemleyip eylemlerin gidiÅŸatÄ±nÄ± belirliyoruz.
    # Eylem durumu ne getirdi? Bu eylemin sonucunda Ã¶dÃ¼l mÃ¼ var?
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # AlÄ±nan bÃ¼tÃ¼n Ã¶dÃ¼l deÄŸerleri deÄŸerlendirilir ve deÄŸerlendirilen Ã¶dÃ¼llerden
    # son duruma ulaÅŸÄ±lÄ±r.
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values

    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Huber loss Ã¶lÃ§Ã¼mÃ¼
    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
    #Optimizer
    optimizer.zero_grad()
    loss.backward()
    # Gradyen azaltma
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    optimizer.step()

"""## DÃ¶ngÃ¼yÃ¼ Ã‡alÄ±ÅŸtÄ±rma"""

# Ä°ÅŸlemci seÃ§imine gÃ¶re dÃ¶ngÃ¼ sayÄ±sÄ± oluÅŸturuluyor.
if torch.cuda.is_available():
    num_episodes = 600
else:
    num_episodes = 50

with VideoRecorder() as video_recorder:
  for i_episode in range(num_episodes):
      # Env oluÅŸturulup durumlar hazÄ±rlanÄ±yor.
      state, info = env.reset()

      #State'in boyutu bir arttÄ±rÄ±lÄ±yor.
      state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)

      for t in count():
          #Video kaydÄ± alma
          video_recorder.record_frame(env)
          #Eylem, gÃ¶zlem, Ã¶dÃ¼l vs. yazdÄ±rÄ±lÄ±yor.
          action = select_action(state)
          observation, reward, terminated, truncated, _ = env.step(action.item())
          reward = torch.tensor([reward], device=device)
          done = terminated or truncated

          if terminated:
              next_state = None
          else:
              next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)

          # Elde edilen deneyi hafÄ±zaya atacaÄŸÄ±z.
          memory.push(state, action, next_state, reward)

          # Sonraki duruma geÃ§
          state = next_state

          # Optimizer ve Loss
          optimize_model()

          # Hafif deÄŸer gÃ¼ncellemesi yapÄ±lacak.
          target_net_state_dict = target_net.state_dict()
          policy_net_state_dict = policy_net.state_dict()
          for key in policy_net_state_dict:
              target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
          target_net.load_state_dict(target_net_state_dict)

          if done:
              episode_durations.append(t + 1)
              plot_durations()
              break

print('Bitti!')
plot_durations(show_result=True)
plt.ioff()
plt.show()